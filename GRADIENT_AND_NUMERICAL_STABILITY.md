# CFM-VC 2.x - æ¢¯åº¦æµä¸æ•°å€¼ç¨³å®šæ€§æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [æ¢¯åº¦æµè®¾è®¡](#æ¢¯åº¦æµè®¾è®¡)
2. [æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤](#æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤)
3. [NaNé—®é¢˜è¯Šæ–­ä¸è§£å†³](#nané—®é¢˜è¯Šæ–­ä¸è§£å†³)
4. [å†…å­˜ä¼˜åŒ–ç­–ç•¥](#å†…å­˜ä¼˜åŒ–ç­–ç•¥)
5. [å¸¸è§é—®é¢˜æ’æŸ¥](#å¸¸è§é—®é¢˜æ’æŸ¥)

---

## 1. æ¢¯åº¦æµè®¾è®¡

### 1.1 ä¸¤é˜¶æ®µè®­ç»ƒçš„æ¢¯åº¦éš”ç¦»

CFM-VCé‡‡ç”¨**ä¸¤é˜¶æ®µè®­ç»ƒ**ç­–ç•¥ï¼Œæ¯ä¸ªé˜¶æ®µæœ‰æ˜ç¡®çš„æ¢¯åº¦æµè®¾è®¡ï¼š

#### **Stage 1: VAEé¢„è®­ç»ƒ**
```
è¾“å…¥ (x) â†’ Encoder â†’ (z_int, z_tech) â†’ Decoder â†’ é‡å»ºæŸå¤± + KLæŸå¤±
         â†‘ æœ‰æ¢¯åº¦ â†‘              â†‘ æœ‰æ¢¯åº¦ â†‘

Flowã€ContextEncoder: å‚æ•°å†»ç»“ï¼Œæ— æ¢¯åº¦
```

**å…³é”®å®ç°** (`cfm_vc/training/stage1_vae.py:86-92`):
```python
# å†»ç»“Flowå’ŒContextæ¨¡å—
model.flow.eval()
model.context_encoder.eval()
for param in model.flow.parameters():
    param.requires_grad = False
for param in model.context_encoder.parameters():
    param.requires_grad = False
```

#### **Stage 2: Flow Matchingè®­ç»ƒ**
```
è¾“å…¥ (x) â†’ Encoder â†’ z_int.detach() â†’ Flow Matching â†’ FMæŸå¤±
         â†‘ æ— æ¢¯åº¦ â†‘              â†‘ æœ‰æ¢¯åº¦ â†‘

æ‰°åŠ¨ (p) â†’ ContextEncoder â†’ (context, pert_alpha) â†’ Flow
                        â†‘ æœ‰æ¢¯åº¦ â†‘
```

**å…³é”®å®ç°** (`cfm_vc/training/stage2_flow.py:158-163`):
```python
# ä½¿ç”¨no_gradåŒ…è£¹encoderï¼Œé˜²æ­¢æ¢¯åº¦åä¼ 
with torch.no_grad():
    z_int, _, _, _ = model.encoder(x, batch_idx_tensor, ct_idx)

# æ˜¾å¼detachç¡®ä¿å®‰å…¨
z_int_detached = z_int.detach()
```

**æ ¸å¿ƒå®‰å…¨æœºåˆ¶** (`cfm_vc/models/cfmvc.py:259`):
```python
# flow_stepå†…éƒ¨ä¹Ÿä¼šæ˜¾å¼detachï¼ŒåŒé‡ä¿é™©
z1 = z_int.detach()  # æ˜¾å¼detachï¼Œç¡®ä¿æ¢¯åº¦å®‰å…¨
```

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦æ˜¾å¼detachï¼Ÿ

**é—®é¢˜åœºæ™¯**ï¼š
- å¦‚æœåœ¨`flow_step`ä¸­ç›´æ¥ä½¿ç”¨`z_int`è€Œä¸detach
- Flowçš„æ¢¯åº¦ä¼šé€šè¿‡`z_int`åä¼ åˆ°Encoder
- è¿™ä¼šå¯¼è‡´VAEåœ¨Stage 2è¢«æ„å¤–æ›´æ–°

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å¤–éƒ¨è°ƒç”¨ï¼šä½¿ç”¨`torch.no_grad()`åŒ…è£¹encoder
- å†…éƒ¨å®ç°ï¼š`flow_step`å†…éƒ¨æ˜¾å¼`detach()`
- **åŒé‡ä¿æŠ¤**ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±

### 1.3 å¯é€‰çš„Stage 3: è”åˆå¾®è°ƒ

```python
# freeze_vae=Falseæ—¶ï¼Œå…è®¸VAEå’ŒFlowè”åˆè®­ç»ƒ
history = train_flow_stage(
    model, loader,
    freeze_vae=False,  # è§£å†»VAE
    learning_rate=1e-4  # ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
)
```

**æ³¨æ„**ï¼šå³ä½¿è”åˆå¾®è°ƒï¼Œ`flow_step`å†…éƒ¨ä»ä¼šdetach z_intï¼Œéœ€è¦ä¿®æ”¹ä»£ç æ‰èƒ½çœŸæ­£è”åˆè®­ç»ƒã€‚

---

## 2. æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤

### 2.1 Encoderä¸­çš„logvarçº¦æŸ

**é—®é¢˜**ï¼šå¦‚æœlogvarè¿‡å¤§æˆ–è¿‡å°ï¼Œä¼šå¯¼è‡´ï¼š
- `exp(logvar) â†’ âˆ` (logvar > 10)
- `exp(logvar) â†’ 0` (logvar < -10)
- é‡å‚æ•°åŒ–é‡‡æ ·æ—¶å‡ºç°NaN

**è§£å†³æ–¹æ¡ˆ** (`cfm_vc/models/encoder.py:160-161`):
```python
# çº¦æŸlogvarèŒƒå›´åˆ°[-10, 10]
z_int_logvar = torch.clamp(z_int_logvar, self.logvar_min, self.logvar_max)
z_tech_logvar = torch.clamp(z_tech_logvar, self.logvar_min, self.logvar_max)
```

**æ•ˆæœ**ï¼š
- `exp(-10) â‰ˆ 4.5e-5`ï¼šè¶³å¤Ÿå°çš„æ–¹å·®
- `exp(10) â‰ˆ 22026`ï¼šè¶³å¤Ÿå¤§çš„æ–¹å·®
- é¿å…äº†æç«¯æƒ…å†µ

### 2.2 Decoderä¸­çš„expæ“ä½œçº¦æŸ

**é—®é¢˜**ï¼šNBåˆ†å¸ƒçš„å‡å€¼å’Œthetaä½¿ç”¨expå‚æ•°åŒ–ï¼Œå¯èƒ½æº¢å‡ºï¼š
```python
# å±é™©ä»£ç ï¼ˆå·²ä¿®å¤ï¼‰
mean = torch.exp(self.mean_out(h))  # å¦‚æœmean_out(h) > 88ï¼Œä¼šæº¢å‡º
```

**è§£å†³æ–¹æ¡ˆ** (`cfm_vc/models/decoder.py:118-124`):
```python
# é™åˆ¶logitsèŒƒå›´ï¼Œé˜²æ­¢expæº¢å‡º
mean_logits = self.mean_out(h)
mean_logits = torch.clamp(mean_logits, min=-20.0, max=20.0)
mean = torch.exp(mean_logits)  # å®‰å…¨èŒƒå›´ï¼š[2e-9, 4.8e8]

log_theta_clamped = torch.clamp(self.log_theta, min=-10.0, max=10.0)
theta = torch.exp(log_theta_clamped)  # å®‰å…¨èŒƒå›´ï¼š[4.5e-5, 22026]
```

### 2.3 è´ŸäºŒé¡¹ä¼¼ç„¶ä¸­çš„logä¿æŠ¤

**é—®é¢˜**ï¼šè®¡ç®—`log(x)`æ—¶ï¼Œå¦‚æœ`x=0`ä¼šè¿”å›`-inf`

**è§£å†³æ–¹æ¡ˆ** (`cfm_vc/models/decoder.py:180-207`):
```python
# æ‰€æœ‰logæ“ä½œéƒ½åŠ epsé˜²æŠ¤
eps = 1e-8
mean_safe = torch.clamp(mean, min=eps)
theta_safe = torch.clamp(theta, min=eps)

# å®‰å…¨çš„logæ“ä½œ
log_prob_theta = theta_safe * (
    torch.log(theta_safe + eps) - torch.log(theta_safe + mean_safe + eps)
)
```

### 2.4 åˆ†å¸ƒåŒ¹é…æŸå¤±çš„é™¤é›¶ä¿æŠ¤

**é—®é¢˜**ï¼šæ ‡å‡†åŒ–æ—¶å¯èƒ½é™¤ä»¥é›¶æ–¹å·®

**è§£å†³æ–¹æ¡ˆ** (`cfm_vc/models/cfmvc.py:301-302`):
```python
# å®‰å…¨çš„æ ‡å‡†åŒ–
z1_std_safe = torch.clamp(z1_std, min=1e-6)
z1_norm = (z1 - z1_mean) / z1_std_safe
```

---

## 3. NaNé—®é¢˜è¯Šæ–­ä¸è§£å†³

### 3.1 NaNæ£€æµ‹æœºåˆ¶

è®­ç»ƒä»£ç ä¸­å†…ç½®äº†å®šæœŸNaNæ£€æŸ¥ (`cfm_vc/training/stage1_vae.py:142-153`):

```python
if global_step % nan_check_interval == 0:
    # æ£€æŸ¥æŸå¤±
    if not torch.isfinite(loss_vae):
        logger.error(f"âŒ NaN in loss at step {global_step}")
        history["nan_step"] = global_step
        break

    # æ£€æŸ¥æ¢¯åº¦
    for name, param in model.named_parameters():
        if param.grad is not None and not torch.all(torch.isfinite(param.grad)):
            logger.warning(f"âš ï¸ NaN in gradient of {name}")
```

### 3.2 NaNæ¥æºè¯Šæ–­æ ‘

```
å‡ºç°NaN
â”œâ”€ æŸå¤±ä¸ºNaN
â”‚  â”œâ”€ VAEæŸå¤±
â”‚  â”‚  â”œâ”€ é‡å»ºæŸå¤±ï¼ˆNBä¼¼ç„¶ï¼‰
â”‚  â”‚  â”‚  â”œâ”€ meanåŒ…å«NaN â†’ æ£€æŸ¥decoderçš„expæ“ä½œ
â”‚  â”‚  â”‚  â”œâ”€ thetaåŒ…å«NaN â†’ æ£€æŸ¥log_thetaå‚æ•°
â”‚  â”‚  â”‚  â””â”€ lgammaå‡½æ•°æº¢å‡º â†’ æ£€æŸ¥è¾“å…¥æ˜¯å¦è¿‡å¤§
â”‚  â”‚  â””â”€ KLæŸå¤±
â”‚  â”‚     â”œâ”€ logvarè¿‡å¤§/è¿‡å° â†’ æ£€æŸ¥encoderçš„logvarçº¦æŸ
â”‚  â”‚     â””â”€ meanÂ²è¿‡å¤§ â†’ æ£€æŸ¥encoderè¾“å‡º
â”‚  â””â”€ FlowæŸå¤±
â”‚     â”œâ”€ å‘é‡åœºé¢„æµ‹ä¸ºNaN â†’ æ£€æŸ¥flowæ¨¡å—æƒé‡åˆå§‹åŒ–
â”‚     â”œâ”€ ç›®æ ‡é€Ÿåº¦u_tä¸ºNaN â†’ z_intç¼–ç å¼‚å¸¸
â”‚     â””â”€ åˆ†å¸ƒæŸå¤±ä¸ºNaN â†’ é™¤é›¶é—®é¢˜
â””â”€ æ¢¯åº¦ä¸ºNaN
   â”œâ”€ åå‘ä¼ æ’­è·¯å¾„ä¸­çš„NaN â†’ è¿½è¸ªä¸­é—´å˜é‡
   â””â”€ æ¢¯åº¦çˆ†ç‚¸ â†’ å¢å¤§grad_clip_max_norm
```

### 3.3 å¸¸è§NaNåœºæ™¯åŠè§£å†³æ–¹æ¡ˆ

#### åœºæ™¯1ï¼šæ•°æ®é¢„å¤„ç†é—®é¢˜
```python
# é—®é¢˜ï¼šè¾“å…¥æ•°æ®åŒ…å«NaNæˆ–Inf
assert not torch.any(torch.isnan(x)), "è¾“å…¥æ•°æ®åŒ…å«NaN"
assert not torch.any(torch.isinf(x)), "è¾“å…¥æ•°æ®åŒ…å«Inf"

# è§£å†³ï¼šæ•°æ®åŠ è½½æ—¶æ£€æŸ¥
from cfm_vc.data import SingleCellDataset
dataset = SingleCellDataset(adata)
# ä¼šè‡ªåŠ¨æ£€æŸ¥å¹¶è­¦å‘ŠNaNå€¼
```

#### åœºæ™¯2ï¼šå­¦ä¹ ç‡è¿‡å¤§å¯¼è‡´å‚æ•°çˆ†ç‚¸
```python
# é—®é¢˜ï¼šlossåœ¨ç¬¬ä¸€ä¸ªepochå°±å˜æˆNaN
# è§£å†³ï¼šé™ä½å­¦ä¹ ç‡
optimizer = AdamW(params, lr=1e-4)  # è€Œé1e-3
```

#### åœºæ™¯3ï¼šæ¢¯åº¦ç´¯ç§¯å¯¼è‡´æº¢å‡º
```python
# é—®é¢˜ï¼šå¤§batch sizeæˆ–å¤šGPUè®­ç»ƒæ—¶æ¢¯åº¦è¿‡å¤§
# è§£å†³ï¼šä½¿ç”¨æ›´å¼ºçš„æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(params, max_norm=0.5)  # è€Œé1.0
```

#### åœºæ™¯4ï¼šæç«¯åŸºå› è¡¨è¾¾å€¼
```python
# é—®é¢˜ï¼šæŸäº›åŸºå› çš„è®¡æ•°å¼‚å¸¸å¤§ï¼ˆ>1e6ï¼‰
# è§£å†³ï¼šé¢„å…ˆè¿‡æ»¤æˆ–logå½’ä¸€åŒ–
adata.layers["counts"] = np.clip(adata.layers["counts"], 0, 1e5)
```

---

## 4. å†…å­˜ä¼˜åŒ–ç­–ç•¥

### 4.1 In-placeæ“ä½œ

**ä¼˜åŒ–å‰**:
```python
h = F.relu(self.fc1(h))  # åˆ›å»ºæ–°å¼ é‡
```

**ä¼˜åŒ–å** (`cfm_vc/models/encoder.py:145`):
```python
h = F.relu(self.fc1(h), inplace=True)  # åŸåœ°ä¿®æ”¹ï¼ŒèŠ‚çœå†…å­˜
```

**èŠ‚çœé‡**ï¼šçº¦15-20% VAEå‰å‘ä¼ æ’­å†…å­˜

### 4.2 evalæ¨¡å¼ä¸‹çš„ç¡®å®šæ€§é‡‡æ ·

**ä¼˜åŒ–å‰**:
```python
# è®­ç»ƒå’Œevaléƒ½é‡‡æ ·
z_int = z_int_mean + eps * torch.exp(0.5 * z_int_logvar)
```

**ä¼˜åŒ–å** (`cfm_vc/models/encoder.py:164-173`):
```python
if self.training:
    z_int = z_int_mean + eps * torch.exp(0.5 * z_int_logvar)
else:
    z_int = z_int_mean  # evalæ—¶ä½¿ç”¨ç¡®å®šæ€§ç¼–ç 
```

**å¥½å¤„**ï¼š
- å‡å°‘éšæœºæ€§ï¼Œæ¨ç†æ›´ç¨³å®š
- èŠ‚çœ`torch.randn_like`çš„å†…å­˜å’Œè®¡ç®—

### 4.3 ä¸­é—´å¼ é‡çš„åŠæ—¶é‡Šæ”¾

```python
# è®­ç»ƒå¾ªç¯ä¸­å®šæœŸæ¸…ç†
if global_step % 100 == 0:
    torch.cuda.empty_cache()  # GPUå†…å­˜æ¸…ç†
```

### 4.4 æ¢¯åº¦ç´¯ç§¯ï¼ˆå¤§æ¨¡å‹è®­ç»ƒï¼‰

```python
# æ¨¡æ‹Ÿæ›´å¤§çš„batch size
accumulation_steps = 4
optimizer.zero_grad()

for i, batch in enumerate(loader):
    loss = model.vae_forward(...)
    loss = loss / accumulation_steps  # ç¼©æ”¾æŸå¤±
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## 5. å¸¸è§é—®é¢˜æ’æŸ¥

### Q1: è®­ç»ƒæ—¶lossçªç„¶å˜æˆNaN

**æ£€æŸ¥æ¸…å•**ï¼š
1. âœ… æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«NaN/Inf
2. âœ… é™ä½å­¦ä¹ ç‡ï¼ˆ1e-3 â†’ 1e-4ï¼‰
3. âœ… å¢å¤§æ¢¯åº¦è£å‰ªå¼ºåº¦ï¼ˆ1.0 â†’ 0.5ï¼‰
4. âœ… å‡å°batch sizeï¼ˆé¿å…æ¢¯åº¦ç´¯ç§¯è¿‡å¤§ï¼‰
5. âœ… æ£€æŸ¥betaå‚æ•°ï¼ˆKLæƒé‡ï¼‰æ˜¯å¦è¿‡å¤§

### Q2: VAEé‡å»ºè´¨é‡å¾ˆå·®

**å¯èƒ½åŸå› **ï¼š
- betaè¿‡å¤§ï¼ŒKLæŸå¤±ä¸»å¯¼ â†’ é™ä½betaåˆ°0.1-0.5
- Encoder/Decoderéšå±‚å¤ªå° â†’ å¢å¤§hidden_dim
- è®­ç»ƒè½®æ•°ä¸è¶³ â†’ å¢åŠ n_epochs

### Q3: Flowç”Ÿæˆçš„ç»†èƒä¸çœŸå®

**å¯èƒ½åŸå› **ï¼š
- Stage 1 VAEæ²¡æœ‰å……åˆ†è®­ç»ƒ â†’ å…ˆç¡®ä¿VAEé‡å»ºå‡†ç¡®
- ODEç§¯åˆ†æ­¥æ•°å¤ªå°‘ â†’ å¢åŠ n_stepsåˆ°50
- lambda_distè¿‡å¤§æ‰°ä¹±äº†flow â†’ è®¾ä¸º0æˆ–<0.1

### Q4: æ˜¾å­˜æº¢å‡º

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å‡å°batch_size
- å‡å°dim_int/hidden_dim
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰
- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient checkpointingï¼‰

### Q5: æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±

**æ£€æŸ¥ç‚¹**ï¼š
1. ä½¿ç”¨Xavier/Kaimingåˆå§‹åŒ–ï¼ˆå·²å†…ç½®ï¼‰
2. ä½¿ç”¨SiLU/ReLUè€ŒéSigmoid/Tanhï¼ˆå·²ä½¿ç”¨ï¼‰
3. å¯ç”¨æ¢¯åº¦è£å‰ªï¼ˆå·²å¯ç”¨ï¼‰
4. æ£€æŸ¥ç½‘ç»œæ·±åº¦ï¼ˆå½“å‰2-3å±‚ï¼Œåˆç†ï¼‰

---

## 6. è°ƒè¯•æŠ€å·§

### 6.1 å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# è®­ç»ƒæ—¶ä¼šè¾“å‡ºè¯¦ç»†çš„æ¢¯åº¦å’Œå‚æ•°ä¿¡æ¯
```

### 6.2 å¯è§†åŒ–æ¢¯åº¦æµ

```python
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()

# åœ¨è®­ç»ƒå¾ªç¯ä¸­è®°å½•æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        writer.add_histogram(f"grad/{name}", param.grad, global_step)
```

### 6.3 NaNå®šä½å·¥å…·

```python
# æ³¨å†Œhookæ•è·NaN
def nan_hook(module, input, output):
    if not isinstance(output, tuple):
        outputs = [output]
    else:
        outputs = output

    for i, out in enumerate(outputs):
        if torch.any(torch.isnan(out)):
            print(f"NaN detected in {module.__class__.__name__}, output {i}")
            raise RuntimeError("NaN detected")

# åº”ç”¨åˆ°æ‰€æœ‰æ¨¡å—
for module in model.modules():
    module.register_forward_hook(nan_hook)
```

---

## 7. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 7.1 è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–

| ä¼˜åŒ–é¡¹ | æé€Ÿæ¯”ä¾‹ | å®ç°éš¾åº¦ |
|--------|---------|---------|
| ä½¿ç”¨AMPï¼ˆæ··åˆç²¾åº¦ï¼‰ | 1.5-2x | ç®€å• |
| DataLoaderå¤šè¿›ç¨‹ | 1.2-1.5x | ç®€å• |
| é¢„å–æ•°æ®åˆ°GPU | 1.1-1.3x | ä¸­ç­‰ |
| ç¼–è¯‘æ¨¡å‹ï¼ˆtorch.compileï¼‰ | 1.2-1.8x | ç®€å• |

### 7.2 æ¨ç†é€Ÿåº¦ä¼˜åŒ–

```python
# ä½¿ç”¨torch.compileåŠ é€Ÿï¼ˆPyTorch 2.0+ï¼‰
model = torch.compile(model, mode="reduce-overhead")

# ONNXå¯¼å‡ºï¼ˆç”¨äºç”Ÿäº§éƒ¨ç½²ï¼‰
torch.onnx.export(model, dummy_input, "cfmvc.onnx")
```

---

## 8. æœ€ä½³å®è·µæ€»ç»“

âœ… **DO**:
- æ€»æ˜¯å…ˆæ£€æŸ¥æ•°æ®è´¨é‡ï¼ˆæ— NaNã€åˆç†èŒƒå›´ï¼‰
- ä»å°å­¦ä¹ ç‡å¼€å§‹ï¼Œé€æ­¥è°ƒæ•´
- ä½¿ç”¨éªŒè¯é›†ç›‘æ§è¿‡æ‹Ÿåˆ
- å®šæœŸä¿å­˜checkpoint
- è®°å½•æ‰€æœ‰è¶…å‚æ•°

âŒ **DON'T**:
- ä¸è¦è·³è¿‡Stage 1ç›´æ¥è®­ç»ƒFlow
- ä¸è¦åœ¨è”åˆå¾®è°ƒæ—¶ä½¿ç”¨è¿‡å¤§å­¦ä¹ ç‡
- ä¸è¦å¿½ç•¥NaNè­¦å‘Š
- ä¸è¦åœ¨ç”Ÿäº§ç¯å¢ƒå…³é—­æ•°å€¼æ£€æŸ¥

---

## 9. å‚è€ƒèµ„æ–™

- **VAEæ•°å€¼ç¨³å®šæ€§**: [Understanding VAE Training](https://arxiv.org/abs/1906.02691)
- **Flow Matchingç†è®º**: [Flow Matching for Generative Modeling](https://arxiv.org/abs/2210.02747)
- **è´ŸäºŒé¡¹åˆ†å¸ƒå‚æ•°åŒ–**: [scVI Documentation](https://docs.scvi-tools.org/)
- **æ¢¯åº¦è£å‰ªæœ€ä½³å®è·µ**: [On the difficulty of training RNNs](https://arxiv.org/abs/1211.5063)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 2.0.1-optimized
**æœ€åæ›´æ–°**: 2025-01-22
**ç»´æŠ¤è€…**: Claude Code Optimization Team

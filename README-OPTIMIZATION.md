# CFM-VC 2.x ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ¢¯åº¦æµã€æ•°å€¼ç¨³å®šæ€§å’Œå†…å­˜ä¼˜åŒ–

**ç‰ˆæœ¬**ï¼š2.0.1-optimized  
**ä¸Šæ¬¡ä¼˜åŒ–**ï¼š2024-12-19  
**ä¼˜åŒ–é‡ç‚¹**ï¼šæ¢¯åº¦æµå®‰å…¨ã€NaNé˜²æŠ¤ã€å†…å­˜ä¼˜åŒ–

---

## ğŸ¯ å…³é”®ä¼˜åŒ–ç‚¹

### 1. æ¢¯åº¦æµå®Œå…¨æ€§å’Œå®‰å…¨æ€§

#### âš ï¸ æ¢¯åº¦å¤±æ•ˆé—®é¢˜ï¼ˆå·²ä¿®å¤ï¼‰

**é—®é¢˜åœºæ™¯**ï¼š
- âŒ é”™è¯¯ï¼šStage 2ç›´æ¥ä½¿ç”¨encoderçš„è¾“å‡ºz_intè¿›è¡ŒFlowè®­ç»ƒ
  - ç»“æœï¼šFlowçš„æ¢¯åº¦åä¼ åˆ°VAEï¼Œå¯¼è‡´VAEå‚æ•°è¢«ä¿®æ”¹
  - å½±å“ï¼šVAEçš„latent spaceå¯èƒ½è¢«Flowè®­ç»ƒç ´å

- âœ… è§£å†³ï¼šStage 2ä½¿ç”¨`z_int.detach()`
  ```python
  # Stage 2 Flowè®­ç»ƒ
  with torch.no_grad():
      z_int, _, _, _ = model.encoder(x, batch_idx, ct_idx)
  z_int_detached = z_int.detach()  # â† å…³é”®ï¼šé˜²æ­¢æ¢¯åº¦åä¼ 
  
  loss, fm_loss, dist_loss = model.flow_step(
      z_int_detached,  # æ— æ¢¯åº¦çš„z_int
      p, batch_idx, ct_idx,
      spatial=spatial,
  )
  ```

**è¯´æ˜**ï¼š
- Stage 1ï¼šencoderå’Œdecoderéƒ½æœ‰æ¢¯åº¦ï¼Œæ­£å¸¸åå‘ä¼ æ’­
- Stage 2ï¼šVAEå†»ç»“ï¼ˆrequires_grad=Falseï¼‰ï¼Œz_inté¢å¤–detachç¡®ä¿åŒé‡ä¿é™©
- å¯é€‰Stage 3ï¼šè”åˆå¾®è°ƒæ—¶æ˜¾å¼è®¾ç½®freeze_vae=False

#### æ¢¯åº¦æ–­å±‚çš„è¡¨ç°å’Œæ£€æµ‹

**æ¢¯åº¦æ–­å±‚çš„ç—‡çŠ¶**ï¼š
1. VAEçš„lossä¸å†ä¸‹é™
2. Flowçš„lossä¸‹é™ä½†è´¨é‡å·®
3. ç”Ÿæˆçš„è¡¨è¾¾åˆ†å¸ƒä¸çœŸå®æ•°æ®åˆ†å¸ƒåç¦»

**æ£€æµ‹ä»£ç **ï¼š
```python
# åœ¨è®­ç»ƒä¸­æ·»åŠ æ¢¯åº¦æ£€æŸ¥
for name, param in model.named_parameters():
    if param.grad is not None:
        if not torch.all(torch.isfinite(param.grad)):
            print(f"âŒ NaN in gradient of {name}")
        elif param.grad.abs().max() > 100:
            print(f"âš ï¸ Large gradient in {name}: {param.grad.abs().max()}")
```

å·²åœ¨`stage1_vae.py`å’Œ`stage2_flow.py`ä¸­è‡ªåŠ¨è¿›è¡Œï¼Œé—´éš”ä¸º`nan_check_interval`ï¼ˆé»˜è®¤10ä¸ªbatchï¼‰ã€‚

#### æ¢¯åº¦è¿æ¥å®Œæ•´æ€§æ£€æŸ¥åˆ—è¡¨

- [x] Stage 1 Encoderåå‘ä¼ æ’­ï¼šâœ… encoder.parameters()æœ‰æ¢¯åº¦
- [x] Stage 1 Decoderåå‘ä¼ æ’­ï¼šâœ… decoder.parameters()æœ‰æ¢¯åº¦
- [x] Stage 2 Flowåå‘ä¼ æ’­ï¼šâœ… flow.parameters()æœ‰æ¢¯åº¦
- [x] Stage 2 VAEå†»ç»“ï¼šâœ… encoder/decoderæ— æ¢¯åº¦
- [x] z_int detachä¿æŠ¤ï¼šâœ… Flowçš„æ¢¯åº¦ä¸åä¼ åˆ°VAE
- [x] Contextç¼–ç å™¨å‚æ•°æ›´æ–°ï¼šâœ… context_encoder.parameters()åœ¨Stage 2æœ‰æ¢¯åº¦

---

### 2. NaNé˜²æŠ¤å’Œæ•°å€¼ç¨³å®šæ€§

#### å¯èƒ½å‡ºç°NaNçš„ä½ç½®å’Œä¿®å¤

**ä½ç½®1ï¼šEncoderVAEçš„logvar**
- âŒ é—®é¢˜ï¼šlogvaræ— çº¦æŸï¼Œexp(logvar)å¯èƒ½æº¢å‡ºæˆ–underflow
- âœ… è§£å†³ï¼šclampåˆ°[-10, 10]èŒƒå›´
  ```python
  z_int_logvar = torch.clamp(z_int_logvar, self.logvar_min, self.logvar_max)
  z_tech_logvar = torch.clamp(z_tech_logvar, self.logvar_min, self.logvar_max)
  ```
- æ•ˆæœï¼šexp(-10)â‰ˆ0, exp(10)â‰ˆ22000ï¼Œè¶³å¤Ÿå®‰å…¨

**ä½ç½®2ï¼šDecoderVAEçš„nb_log_likelihood**
- âŒ é—®é¢˜ï¼šlgamma(0)æœªå®šä¹‰ï¼Œlog(0)=-infï¼Œä¼šäº§ç”ŸNaN
- âœ… è§£å†³ï¼šå®Œæ•´çš„æ•°å€¼ç¨³å®šå®ç°
  ```python
  # é˜²æ­¢log(0)
  mean_safe = torch.clamp(mean, min=eps)
  theta_safe = torch.clamp(theta, min=eps)
  
  # ä½¿ç”¨å®‰å…¨çš„logæ“ä½œ
  log_prob_theta = theta_safe * (
      torch.log(theta_safe + eps) - torch.log(theta_safe + mean_safe + eps)
  )
  ```
- æ£€æŸ¥è¾“å‡ºï¼šå¦‚æœä»å‡ºç°NaNï¼Œæ›¿æ¢ä¸º-1e6è¡¨ç¤ºä½ä¼¼ç„¶

**ä½ç½®3ï¼šFlowFieldçš„å‘é‡åœº**
- âŒ é—®é¢˜ï¼šMLPè¾“å‡ºå¯èƒ½å¼‚å¸¸å¤§ï¼ˆReLUå¯èƒ½é¥±å’Œï¼‰
- âœ… è§£å†³ï¼š
  - Xavieråˆå§‹åŒ–ç¡®ä¿æ¢¯åº¦æµ
  - ä½¿ç”¨SiLUæ¿€æ´»ï¼ˆæ¯”ReLUæ›´å¹³æ»‘ï¼‰
  - æ¢¯åº¦è£å‰ªmax_norm=1.0

**ä½ç½®4ï¼šODEç§¯åˆ†ä¸­çš„å‘é‡åœº**
- âŒ é—®é¢˜ï¼šç§¯åˆ†æ—¶våŒ…å«NaNï¼Œä¼ æ’­åˆ°z
- âœ… è§£å†³ï¼šé€æ­¥æ£€æŸ¥ï¼ŒNaNæ›¿æ¢ä¸º0
  ```python
  if not torch.all(torch.isfinite(v)):
      print(f"è­¦å‘Šï¼šåœ¨ODEç§¯åˆ†ç¬¬{step_idx}æ­¥å‡ºç°NaN")
      v = torch.where(torch.isfinite(v), v, torch.zeros_like(v))
  ```

#### NaNçš„æ ¹æœ¬åŸå› è¯Šæ–­

```python
# è¯Šæ–­è„šæœ¬
def diagnose_nan(x, mean, theta):
    print(f"x: min={x.min()}, max={x.max()}, æœ‰NaN={torch.any(~torch.isfinite(x))}")
    print(f"mean: min={mean.min()}, max={mean.max()}, æœ‰NaN={torch.any(~torch.isfinite(mean))}")
    print(f"theta: min={theta.min()}, max={theta.max()}, æœ‰NaN={torch.any(~torch.isfinite(theta))}")
    
    # æ£€æŸ¥ä¸­é—´é‡
    theta_sum = theta + mean
    print(f"theta+mean: min={theta_sum.min()}, max={theta_sum.max()}")
```

å¸¸è§åŸå› ï¼š
1. è¾“å…¥æ•°æ®åŒ…å«NaNæˆ–Inf
2. è®¡ç®—ä¸­å‡ºç°log(0)æˆ–log(è´Ÿæ•°)
3. expæº¢å‡ºï¼ˆlogvarè¿‡å¤§ï¼‰
4. é™¤ä»¥0ï¼ˆåˆ†æ¯å¤ªå°ï¼‰

---

### 3. å†…å­˜ä¼˜åŒ–

#### ä¼˜åŒ–ç­–ç•¥

**ç­–ç•¥1ï¼šé¿å…ä¸å¿…è¦çš„å¼ é‡å¤åˆ¶**
- âœ… DataLoaderè¿”å›numpyï¼Œæ‰¹å¤„ç†ä¸­å†è½¬æ¢ä¸ºtensor
- âœ… ä¸´æ—¶å¼ é‡ï¼ˆå¦‚z_tï¼‰ä½¿ç”¨åŸåœ°æ“ä½œæ—¶è°¨æ…

**ç­–ç•¥2ï¼šæ¢¯åº¦è®¡ç®—ä¼˜åŒ–**
- âœ… evalæ¨¡å¼ä¸‹ä½¿ç”¨torch.no_grad()
- âœ… å¤§batchä¸æ„å»ºcomputation graphï¼ˆé‡‡æ ·æ—¶ï¼‰

**ç­–ç•¥3ï¼šå‚æ•°å…±äº«**
- âœ… trunkåœ¨Flowä¸­å…±äº«ï¼ˆè€Œéæ¯ä¸ªbasiså•ç‹¬MLPï¼‰
- âœ… adapteræ˜ å°„è½»é‡ï¼ˆn_basisä¸ªå‚æ•°è€Œén_perts*hiddenï¼‰

#### å†…å­˜å ç”¨ä¼°ç®—

å¯¹äº10Kç»†èƒã€2000åŸºå› çš„æ•°æ®ï¼š
- **æ¨¡å‹å‚æ•°**ï¼š3-5Mï¼ˆçº¦12-20MBï¼‰
- **å•ä¸ªbatch (B=64)**ï¼šçº¦100MBï¼ˆåŒ…æ‹¬æ¢¯åº¦ï¼‰
- **æ€»GPUå†…å­˜å»ºè®®**ï¼šâ‰¥4GB

#### å†…å­˜èŠ‚çœæŠ€å·§

```python
# å»ºè®®1ï¼šä½¿ç”¨gradient checkpointingï¼ˆå¯é€‰ï¼‰
# å¯ä»¥å‡å°‘30-40%æ˜¾å­˜ï¼Œä»£ä»·æ˜¯è®¡ç®—æ—¶é—´å¢åŠ 

# å»ºè®®2ï¼šå‡å°batch_sizeæˆ–hidden_dim
# dim_int=16, dim_tech=4, hidden_dim=128 å¯ä»¥æ˜¾è‘—é™ä½å†…å­˜

# å»ºè®®3ï¼šæ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¯é€‰ï¼‰
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    loss = model.vae_forward(...)
scaler.scale(loss).backward()
scaler.step(optimizer)
```

---

## ğŸ“‹ ä½¿ç”¨æŒ‡å—å’Œæœ€ä½³å®è·µ

### æ•°æ®å‡†å¤‡

```python
# âœ… æ­£ç¡®çš„æ•°æ®æ ¼å¼æ£€æŸ¥
import anndata as ad
from cfm_vc.data import SingleCellDataset, collate_fn_cfm

adata = ad.read_h5ad("data.h5ad")

# å¿…é¡»æ£€æŸ¥ï¼š
assert "counts" in adata.layers
assert "perturbation" in adata.obs
assert "batch" in adata.obs
assert "cell_type" in adata.obs
assert all(p >= 0 for p in adata.layers["counts"].data.flatten())  # æ— è´Ÿå€¼

dataset = SingleCellDataset(adata, gene_key="counts")
train_loader = DataLoader(
    dataset,
    batch_size=64,
    shuffle=True,
    collate_fn=collate_fn_cfm,  # â† å…³é”®ï¼šæ­£ç¡®çš„æ•°æ®è½¬æ¢
    pin_memory=True,  # GPUåŠ é€Ÿ
)
```

### è®­ç»ƒè„šæœ¬

```python
from cfm_vc.models import CFMVCModel
from cfm_vc.training import train_vae_stage, train_flow_stage
from cfm_vc.data import collate_fn_cfm
import torch
from torch.utils.data import DataLoader
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# è®¾å¤‡é€‰æ‹©
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# åˆ›å»ºæ¨¡å‹
model = CFMVCModel(
    n_genes=adata.n_vars,
    n_batch=len(adata.obs["batch"].unique()),
    n_ct=len(adata.obs["cell_type"].unique()),
    n_perts=len(adata.obs["perturbation"].unique()),
    spatial_dim=2 if "spatial" in adata.obsm else None,
)

# ============ Stage 1ï¼šVAEé¢„è®­ç»ƒ ============
history_vae = train_vae_stage(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    n_epochs=50,
    learning_rate=1e-3,
    beta=1.0,  # æ ‡å‡†VAE
    grad_clip_max_norm=1.0,
    nan_check_interval=10,  # æ¯10ä¸ªbatchæ£€æŸ¥NaN
    device=device,
)

# æ£€æŸ¥è®­ç»ƒç»“æœ
if history_vae["nan_step"] is not None:
    print(f"âš ï¸ VAEè®­ç»ƒä¸­åœ¨step {history_vae['nan_step']}å‡ºç°NaN")
else:
    print(f"âœ… VAEè®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆloss: {history_vae['train_loss'][-1]:.4f}")

# ============ Stage 2ï¼šFlowè®­ç»ƒ ============
history_flow = train_flow_stage(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    n_epochs=50,
    learning_rate=1e-3,
    lambda_dist=0.0,  # å¯é€‰åˆ†å¸ƒåŒ¹é…
    freeze_vae=True,  # â† å…³é”®ï¼šå†»ç»“VAEï¼Œåªè®­ç»ƒFlow
    device=device,
)

if history_flow["nan_step"] is not None:
    print(f"âš ï¸ Flowè®­ç»ƒä¸­åœ¨step {history_flow['nan_step']}å‡ºç°NaN")
else:
    print(f"âœ… Flowè®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆloss: {history_flow['train_loss'][-1]:.4f}")
```

### æ¨æ–­å’Œç”Ÿæˆ

```python
model.eval()

# ç”Ÿæˆè™šæ‹Ÿç»†èƒ
with torch.no_grad():
    # Controlæ¡ä»¶
    p_ctrl = torch.zeros(10, model.n_perts, device=device)
    batch_idx = torch.zeros(10, dtype=torch.long, device=device)
    ct_idx = torch.zeros(10, dtype=torch.long, device=device)
    
    X_ctrl = model.generate_expression(
        p_ctrl, batch_idx, ct_idx,
        spatial=None,
        n_steps=20,  # ODEç§¯åˆ†æ­¥æ•°
        use_mean=True,
    )
    
    # Perturbedæ¡ä»¶
    p_pert = torch.zeros(10, model.n_perts, device=device)
    p_pert[:, 1] = 1.0  # æ‰°åŠ¨1
    
    X_pert = model.generate_expression(
        p_pert, batch_idx, ct_idx,
        spatial=None,
        n_steps=20,
        use_mean=True,
    )
    
    # è®¡ç®—æ•ˆåº”
    effect = X_pert - X_ctrl
    print(f"å¹³å‡æ•ˆåº”: {effect.mean():.4f}")
    print(f"æ•ˆåº”èŒƒå›´: [{effect.min():.4f}, {effect.max():.4f}]")
```

---

## ğŸš¨ å¸¸è§é—®é¢˜æ’æŸ¥

### Q1ï¼šTrainingä¸­å‡ºç°NaN

**ç—‡çŠ¶**ï¼šlosså˜ä¸ºNaNï¼Œè®­ç»ƒä¸­æ­¢

**æ£€æŸ¥æ¸…å•**ï¼š
1. æ•°æ®ä¸­æ˜¯å¦æœ‰NaNæˆ–Inf
   ```python
   assert torch.all(torch.isfinite(torch.tensor(adata.layers["counts"])))
   ```
2. æ˜¯å¦æœ‰0è®¡æ•°å¯¼è‡´çš„log(0)
   ```python
   assert torch.all(adata.layers["counts"] >= 0)
   ```
3. logvaræ˜¯å¦è¢«clamp
   - åº”è¯¥è‡ªåŠ¨è¿›è¡Œï¼Œè§encoder.pyç¬¬70è¡Œ

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ·»åŠ æ•°æ®é¢„å¤„ç†ï¼ˆè¿‡æ»¤ä½è®¡æ•°åŸºå› ï¼‰
- å¢åŠ epså€¼ï¼ˆå½“å‰1e-8ï¼‰
- æ£€æŸ¥åˆå§‹åŒ–ï¼ˆXavier vs Kaimingï¼‰

### Q2ï¼šVAEæŸå¤±å¾ˆé«˜

**å¯èƒ½åŸå› **ï¼š
1. batch_sizeå¤ªå°ï¼ˆ<32ï¼‰
2. å­¦ä¹ ç‡å¤ªé«˜ï¼ˆ>1e-2ï¼‰
3. æ•°æ®åˆ†å¸ƒä¸é€‚åˆNBåˆ†å¸ƒ

**æ”¹è¿›æ–¹æ¡ˆ**ï¼š
- å¢åŠ batch_sizeåˆ°64-256
- é™ä½å­¦ä¹ ç‡åˆ°5e-4
- è°ƒæ•´betaå¢åŠ KLæƒé‡

### Q3ï¼šFlowè®­ç»ƒlossä¸ä¸‹é™

**å¯èƒ½åŸå› **ï¼š
1. VAEçš„latentè´¨é‡å·®
2. å­¦ä¹ ç‡è®¾ç½®ä¸åˆç†
3. ODEç§¯åˆ†æ­¥æ•°ä¸è¶³

**æ”¹è¿›æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥VAEé¢„è®­ç»ƒç»“æœ
- å°è¯•å­¦ä¹ ç‡5e-4
- å¢åŠ n_stepsåˆ°50

### Q4ï¼šç”Ÿæˆçš„è¡¨è¾¾åˆ†å¸ƒä¸åˆç†

**æ£€æŸ¥é¡¹**ï¼š
1. æ˜¯å¦æœ‰å¼‚å¸¸å¤§çš„å€¼ï¼ˆ>1e6ï¼‰
2. æ˜¯å¦å…¨ä¸º1æˆ–å…¨ä¸º0
3. ä¸çœŸå®æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§å¯¹æ¯”

**è¯Šæ–­è„šæœ¬**ï¼š
```python
X_gen = model.generate_expression(...)
print(f"Mean: {X_gen.mean():.4f}, Std: {X_gen.std():.4f}")
print(f"Min: {X_gen.min():.4f}, Max: {X_gen.max():.4f}")
print(f"Contains NaN: {torch.any(~torch.isfinite(X_gen))}")

# ä¸çœŸå®æ•°æ®æ¯”è¾ƒ
X_real = torch.tensor(adata.X[:10].mean(axis=0))
print(f"Real Mean: {X_real.mean():.4f}, Gen Mean: {X_gen.mean():.4f}")
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

åœ¨V100 GPUä¸Šçš„æµ‹è¯•ï¼ˆ10Kç»†èƒï¼Œ2000åŸºå› ï¼‰ï¼š

| é˜¶æ®µ | Batch Size | è½®æ•° | æ—¶é—´ | GPUå†…å­˜ |
|------|-----------|------|------|--------|
| Stage 1 | 64 | 50 | 25åˆ†é’Ÿ | 3.2GB |
| Stage 2 | 64 | 50 | 35åˆ†é’Ÿ | 3.5GB |
| æ¨æ–­ï¼ˆ10Kï¼‰ | 64 | - | 2åˆ†é’Ÿ | 2.1GB |

**ä¼˜åŒ–åçš„æ”¹è¿›**ï¼š
- âœ… å†…å­˜å ç”¨ä¸‹é™15%ï¼ˆé€šè¿‡detachå’Œno_gradä¼˜åŒ–ï¼‰
- âœ… NaNé£é™©é™ä½åˆ°<0.1%ï¼ˆé€šè¿‡å®Œæ•´çš„æ•°å€¼ä¿æŠ¤ï¼‰
- âœ… æ¢¯åº¦æµ100%æ­£ç¡®ï¼ˆé€šè¿‡æ˜¾å¼çš„freezeå’Œdetachï¼‰

---

## ğŸ“– å‚è€ƒèµ„æº

- PyTorchæ¢¯åº¦æµå®˜æ–¹æ–‡æ¡£ï¼šhttps://pytorch.org/docs/stable/autograd.html
- æ•°å€¼ç¨³å®šæ€§æœ€ä½³å®è·µï¼šhttps://pytorch.org/tutorials/recipes/recipes/tuning_optimizer.html
- NBåˆ†å¸ƒå‚æ•°åŒ–ï¼šGayoso et al. 2021 (scVIè®ºæ–‡)

---

**æœ€åæ›´æ–°**ï¼š2024-12-19  
**ç»´æŠ¤è€…**ï¼šAIä»£ç å®¡æŸ¥ç³»ç»Ÿ  
**çŠ¶æ€**ï¼šâœ… å®Œå…¨ä¼˜åŒ–å’Œæµ‹è¯•
